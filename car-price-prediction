{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9703535,"sourceType":"datasetVersion","datasetId":5934261}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ayushgharat17/kagglex-skill-assessment-challenge?scriptVersionId=202872714\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# KaggleX Skill Assessment Challenge: Car Price Prediction\n\nWelcome to my notebook for the KaggleX Skill Assessment Challenge, where the goal is to predict car prices based on various features. This notebook documents my approach to analyzing the data, engineering features, and building a predictive model to estimate car prices accurately.\n\n## Objective:\nThe main goal of this competition is to predict the price of cars using features such as brand, mileage, transmission, and more.\n\n## Steps:\n1. **Data Exploration**: We'll explore the dataset and understand the key features that might impact car prices.\n2. **Data Preprocessing**: We'll clean and prepare the data for modeling.\n3. **Modeling**: We'll build various machine learning models to predict the car prices.\n4. **Model Evaluation**: We'll compare the models using performance metrics and select the best one.\n\nLet’s dive into the data!\n\n## Dataset Overview\n\nThe dataset consists of the following columns:\n\n- **`id`**: A unique identifier for each car entry.\n- **`brand`**: The brand or manufacturer of the car (e.g., Toyota, BMW, Ford).\n- **`model`**: The specific model of the car (e.g., Corolla, 3 Series, Mustang).\n- **`transmission`**: The type of transmission used in the car (e.g., Manual, Automatic).\n- **`engine`**: The engine specification, often including displacement or type (e.g., 2.0L, V6).\n- **`mileage`**: The distance the car has traveled, typically measured in kilometers or miles.\n- **`model_year`**: The year the car model was manufactured.\n- **`fuel_type`**: The type of fuel the car uses (e.g., Petrol, Diesel, Electric).\n- **`int_col`**: The interior color of the car.\n- **`ext_col`**: The exterior color of the car.\n- **`accident`**: A binary indicator (0 or 1) showing whether the car has had accidents.\n- **`price`**: The price of the car (target variable).\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T09:28:03.054447Z","iopub.execute_input":"2024-06-13T09:28:03.054826Z","iopub.status.idle":"2024-06-13T09:28:03.059269Z","shell.execute_reply.started":"2024-06-13T09:28:03.054798Z","shell.execute_reply":"2024-06-13T09:28:03.058322Z"}}},{"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport warnings\nfrom scipy.stats import skew\n\n# Machine learning imports\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom category_encoders import TargetEncoder\n\n# XGBoost import\nfrom xgboost import XGBRegressor\n\n# Warnings and plotting settings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\n# Importing the Data\ntrain = pd.read_csv(\"/kaggle/input/car-price-dataset/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/car-price-dataset/test.csv\")\noriginal = pd.read_csv(\"/kaggle/input/car-price-dataset/used_cars.csv\")  # This is the true dataset from Kaggle named Used-Car Price\n\n# Replacing unnecessary columns(we have deleted the column 'clean_title' as it lack potential to show variation due to monotonicity of the values)\ntrain.drop(columns=[\"id\",\"clean_title\"],inplace=True) \ntest.drop(columns=[\"id\",\"clean_title\"],inplace=True)\n\n# Check for successfull data loading\nfor dirname,_,filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname,filename))                         ","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:21.486162Z","iopub.execute_input":"2024-10-23T15:44:21.486663Z","iopub.status.idle":"2024-10-23T15:44:24.432648Z","shell.execute_reply.started":"2024-10-23T15:44:21.486616Z","shell.execute_reply":"2024-10-23T15:44:24.431389Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Performing the transformation and formatting in the orginal to match with train data for successfull and valid concatenation\n# Drop the column 'clean_title'\noriginal.drop(\"clean_title\",axis=1,inplace=True) \n\n# Converting the noisy string to a well-defined format (parsing numeric values from strings)\noriginal['milage'] = original['milage'].str.replace(',', '').str.replace(' mi', '').astype(float)\noriginal['price'] = original['price'].str.replace('$', '').str.replace(',', '').astype(float)\n\n# Concatenating the original and traib data\ntrain = pd.concat([train,original],ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:41.851952Z","iopub.execute_input":"2024-10-23T15:44:41.852412Z","iopub.status.idle":"2024-10-23T15:44:41.884781Z","shell.execute_reply.started":"2024-10-23T15:44:41.852369Z","shell.execute_reply":"2024-10-23T15:44:41.883118Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Exploration\n\nIn this section, we'll take a closer look at the dataset. We'll visualize key variables, check for missing values, and summarize the data to understand its structure.","metadata":{}},{"cell_type":"code","source":"# Display the first few rows of the dataset to understand structure and content\nprint(\"First 5 Rows of the Dataset:\\n\", train.head(), \"\\n\")\n\n# Summary statistics of the dataset for numerical columns\nprint(\"Summary Statistics for Numerical Columns:\\n\", train.describe(), \"\\n\")\n\n# Checking for missing values in each column to identify any gaps in the data\nmissing_values = train.isnull().sum()\nprint(\"Missing Values in Each Column:\\n\", missing_values[missing_values > 0], \"\\n\")\n\n# Counting the number of unique values in categorical columns to understand their diversity\nunique_values = train.nunique()\nprint(\"Number of Unique Values in Each Column:\\n\", unique_values, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:41.887597Z","iopub.execute_input":"2024-10-23T15:44:41.88811Z","iopub.status.idle":"2024-10-23T15:44:41.999036Z","shell.execute_reply.started":"2024-10-23T15:44:41.888057Z","shell.execute_reply":"2024-10-23T15:44:41.997676Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Key Features\n\nNow let's visualize some key features of the dataset to gain more insights into their distributions and relationships.","metadata":{}},{"cell_type":"code","source":"# Visualizing the distribution of car prices\n# This helps us understand the range and frequency of car prices\nplt.figure(figsize=(10, 6))\nsns.histplot(train['price'], bins=30, kde=True)\nplt.title('Distribution of Car Prices')\nplt.xlabel('Price')\nplt.ylabel('Frequency')\nplt.show()\n\n# Visualizing the count of cars by brand\n# Helps us see the distribution of brands in the dataset\nplt.figure(figsize=(10,9))\nsns.countplot(y='brand', data=train, order=train['brand'].value_counts().index)\nplt.title('Car Brand Frequency')\nplt.show()\n\n# Correlation heatmap\n# This visualization shows the relationships between different numerical features\nplt.figure(figsize=(10, 8))\nsns.heatmap(train.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:42.000846Z","iopub.execute_input":"2024-10-23T15:44:42.001372Z","iopub.status.idle":"2024-10-23T15:44:44.025173Z","shell.execute_reply.started":"2024-10-23T15:44:42.00132Z","shell.execute_reply":"2024-10-23T15:44:44.023973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing\n\nIn this section, we'll clean and preprocess the dataset by:\n- Handling missing values.\n- Encoding categorical features.\n- Normalizing/standardizing numerical features (if needed).","metadata":{}},{"cell_type":"markdown","source":"After conducting a thorough analysis of the dataset, I identified that the missing values are represented by the character '–'. In the next step, I will replace these occurrences of '–' with a more appropriate representation of missing data. Following this replacement, I will perform a comprehensive assessment of the dataset to analyze any remaining null values.","metadata":{}},{"cell_type":"code","source":"# Replacing '–' with default numpy null (np.Nan)\ntrain.replace('–',np.nan,inplace=True)\ntest.replace('–',np.nan,inplace=True)\n\n# Viewing the nuber of null values after the replacement\nprint(\"Missing values in the train data:\")\nprint(train.isna().sum())\nprint()\nprint(\"---------------------------------------------------\")\nprint()\nprint(\"Missing values in the test data:\")\nprint(test.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:44.026787Z","iopub.execute_input":"2024-10-23T15:44:44.027292Z","iopub.status.idle":"2024-10-23T15:44:44.134294Z","shell.execute_reply.started":"2024-10-23T15:44:44.027243Z","shell.execute_reply":"2024-10-23T15:44:44.133013Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputation of Missing Values in the 'transmission' Column\n\nUpon analyzing the dataset, it was observed that the `transmission` column contained 11 missing values in the training data and 3 missing values in the test data. To address these missing values, I opted for a simple yet effective imputation strategy by utilizing the mode of the `transmission` column.\n\nThe rationale for choosing mode-based imputation includes:\n\n1. **Simplicity**: The mode is the most frequently occurring value in the dataset. By replacing the missing values with the mode, we ensure that the imputed values are representative of the existing data distribution.\n\n2. **Preservation of Data Integrity**: Using the mode minimizes the introduction of bias into the dataset, as it reflects the most common transmission type observed.\n\n3. **Consistency Across Datasets**: By applying the same mode value from the training set to both the training and test sets, we maintain consistency and avoid any potential discrepancies that could arise from different imputation strategies.\n\nThus, the missing values in the `transmission` column have been replaced as follows:","metadata":{}},{"cell_type":"code","source":"# Finding the mode of the values in 'transmission' column\ntransmission_mode = train['transmission'].mode()[0]\n\n# Replacing the null values with the mode (in both train & test data)\ntrain['transmission'] = train['transmission'].fillna(transmission_mode)\ntest['transmission'] = test['transmission'].fillna(transmission_mode)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:44.137107Z","iopub.execute_input":"2024-10-23T15:44:44.137518Z","iopub.status.idle":"2024-10-23T15:44:44.158452Z","shell.execute_reply.started":"2024-10-23T15:44:44.137479Z","shell.execute_reply":"2024-10-23T15:44:44.157051Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputation of Missing and Unsupported Values in the 'fuel_type' Column\n\nIn the process of data cleaning and preparation, I identified that the `fuel_type` column contained both missing values (represented as `NaN`) and instances of the value \"not supported.\" After thorough research and documentation regarding the dataset and the context of the problem, I determined that it would be appropriate to replace these values with \"Electric.\"\n\nThe decision to impute these values with \"Electric\" is based on the following considerations:\n\n1. **Market Trends**: Recent automotive industry trends indicate a significant shift towards electric vehicles (EVs) as a viable and sustainable alternative. By categorizing unsupported or missing fuel types as \"Electric,\" we align the dataset with contemporary market dynamics.\n\n2. **Data Consistency**: Replacing missing and unsupported values with a single, coherent category (i.e., \"Electric\") enhances the uniformity of the dataset. This consistency is essential for subsequent analysis and modeling, reducing noise that could arise from undefined or inconsistent categories.\n\n3. **Preservation of Information**: By imputing these values with \"Electric,\" we are not introducing arbitrary categories but instead opting for a category that is relevant and increasingly prevalent. This approach helps maintain the integrity of the dataset while providing meaningful insights.\n\nThe imputation process was executed as follows:","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:10:33.692053Z","iopub.execute_input":"2024-06-14T04:10:33.692458Z","iopub.status.idle":"2024-06-14T04:10:33.700019Z","shell.execute_reply.started":"2024-06-14T04:10:33.692431Z","shell.execute_reply":"2024-06-14T04:10:33.698473Z"}}},{"cell_type":"code","source":"# Unique values in 'fuel_type' column\nprint(f\"The number of unique fuel types available for the car are: {train['fuel_type'].nunique()} \\n\")\nprint(\"The count of those unique values:\")\nprint(train[\"fuel_type\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:44.160355Z","iopub.execute_input":"2024-10-23T15:44:44.160785Z","iopub.status.idle":"2024-10-23T15:44:44.177052Z","shell.execute_reply.started":"2024-10-23T15:44:44.160741Z","shell.execute_reply":"2024-10-23T15:44:44.175653Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace NaN and 'not supported' values with 'Electric' in the training and test datasets\ntrain[\"fuel_type\"] = train[\"fuel_type\"].replace(np.nan,\"Electric\")\ntest[\"fuel_type\"] = test[\"fuel_type\"].replace(np.nan,\"Electric\")\ntrain[\"fuel_type\"] = train[\"fuel_type\"].replace(\"not supported\",\"Electric\")\ntest[\"fuel_type\"] = test[\"fuel_type\"].replace(\"not supported\",\"Electric\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:44.17885Z","iopub.execute_input":"2024-10-23T15:44:44.179328Z","iopub.status.idle":"2024-10-23T15:44:44.215427Z","shell.execute_reply.started":"2024-10-23T15:44:44.17926Z","shell.execute_reply":"2024-10-23T15:44:44.214072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputation of Missing Values in 'int_col' and 'ext_col'\n\nAfter conducting a detailed analysis of the `int_col` (interior color) and `ext_col` (exterior color) columns, I observed a significant trend in the frequency of color selections within the dataset. The counts revealed that \"Black\" is the most prevalent color by a considerable margin, making it a strong candidate for imputing the missing values. Although \"White\" also ranks high in frequency, I opted to use \"Black\" as the replacement due to the following reasons:\n\n1. **Market Demand**: Black is consistently favored in the automotive industry, being associated with sophistication and versatility. This popularity reinforces its status as a reliable choice for imputation.\n\n2. **Statistical Significance**: Visualizing the counts of various colors, \"Black\" emerged as the dominant color, suggesting that it reflects the preferences of consumers more accurately than other options.\n\n3. **Consistency Across Attributes**: By choosing \"Black\" for both interior and exterior colors, we ensure consistency in our dataset, which is vital for subsequent analysis and modeling.\n\nThe top colors identified from the analysis are as follows:","metadata":{}},{"cell_type":"code","source":"# Displaying the top colors in the dataset\ntop_interior_colors = train['int_col'].value_counts().head(10)\ntop_exterior_colors = train['ext_col'].value_counts().head(10)\n\nprint(\"Top Interior Colors:\\n\", top_interior_colors,\"\\n\")\nprint(\"Top Exterior Colors:\\n\", top_exterior_colors)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:44.216699Z","iopub.execute_input":"2024-10-23T15:44:44.217115Z","iopub.status.idle":"2024-10-23T15:44:44.234653Z","shell.execute_reply.started":"2024-10-23T15:44:44.217074Z","shell.execute_reply":"2024-10-23T15:44:44.233274Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace NaN values in the interior color and exterior color columns with 'Black'\ntrain[\"int_col\"] = train[\"int_col\"].replace(np.nan, \"Black\")\ntest[\"int_col\"] = test[\"int_col\"].replace(np.nan, \"Black\")\ntrain[\"ext_col\"] = train[\"ext_col\"].replace(np.nan, \"Black\")\ntest[\"ext_col\"] = test[\"ext_col\"].replace(np.nan, \"Black\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:44.236433Z","iopub.execute_input":"2024-10-23T15:44:44.236798Z","iopub.status.idle":"2024-10-23T15:44:44.272051Z","shell.execute_reply.started":"2024-10-23T15:44:44.236758Z","shell.execute_reply":"2024-10-23T15:44:44.270756Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Handling Unseen Categories\n\nIn the course of preparing the dataset for modeling, it became essential to address the presence of unseen categories in the test dataset. Unseen categories are those values that appear in the test set but do not exist in the training set, which can pose challenges during model training and evaluation.\n\n#### Step 1: Identifying Unseen Categories\n\nTo identify unseen categories, I developed a function that counts how many unique values in the test dataset do not exist in the training dataset for specified categorical columns. The columns examined include `brand`, `model`, `fuel_type`, `transmission`, `int_col`, and `ext_col`. The results of this analysis help us understand the extent of the issue before proceeding with imputation.","metadata":{}},{"cell_type":"code","source":"# unseen values \ndef unseen(train_cols, test_cols):\n    un = {}\n    for col in test_cols:\n        un[col] = 0\n        for value in test[col].unique():  # Use test_data here\n            if value not in train[col].unique():  # Use train_data here\n                un[col] += 1\n    return pd.DataFrame({\"Columns\": un.keys(), \"Count\": un.values()})\n\ncols = [\"brand\",\"model\",\"fuel_type\",\"transmission\",\"int_col\",\"ext_col\"]\nprint(unseen(cols,cols))","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:44.273517Z","iopub.execute_input":"2024-10-23T15:44:44.273895Z","iopub.status.idle":"2024-10-23T15:44:54.982015Z","shell.execute_reply.started":"2024-10-23T15:44:44.273854Z","shell.execute_reply":"2024-10-23T15:44:54.980854Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Step 2: Handling Unseen Categories\nAfter identifying the unseen categories, I implemented a function to replace these values in the test dataset with a standardized label, 'Unknown'. This approach ensures that the model can handle any unseen values gracefully without causing errors during predictions.","metadata":{}},{"cell_type":"code","source":"# Function declaration for checking unseen values\ndef handle_unseen_categories(train_col, test_col):\n    known_categories = set(train_col)\n    return test_col.apply(lambda x: x if x in known_categories else 'Unknown')\n\n# Apply the function to each categorical column\ncategorical_columns = [\"brand\", \"model\", \"fuel_type\", \"transmission\", \"int_col\", \"ext_col\", \"accident\"]\nfor col in categorical_columns:\n    train[col] = train[col].astype(str)\n    test[col] = test[col].astype(str)\n    test[col] = handle_unseen_categories(train[col], test[col])\n\n    # Diagnostic print to check for any unknowns\n    unknowns_in_test = test[col][~test[col].isin(set(train[col]))]\n    if not unknowns_in_test.empty:\n        print(f\"Unseen categories in '{col}' after handling: {unknowns_in_test.unique()}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:54.983351Z","iopub.execute_input":"2024-10-23T15:44:54.983689Z","iopub.status.idle":"2024-10-23T15:44:55.16868Z","shell.execute_reply.started":"2024-10-23T15:44:54.983656Z","shell.execute_reply":"2024-10-23T15:44:55.167487Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply the function to each categorical column\ncategorical_columns = [\"brand\", \"model\", \"fuel_type\", \"transmission\", \"int_col\", \"ext_col\", \"accident\"]\nfor col in categorical_columns:\n    train[col] = train[col].astype(str)\n    test[col] = test[col].astype(str)\n    test[col] = handle_unseen_categories(train[col], test[col])\n\n    # Diagnostic print to check for any unknowns\n    unknowns_in_test = test[col][~test[col].isin(set(train[col]))]\n    if not unknowns_in_test.empty:\n        print(f\"Unseen categories in '{col}' after handling: {unknowns_in_test.unique()}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:55.170168Z","iopub.execute_input":"2024-10-23T15:44:55.1706Z","iopub.status.idle":"2024-10-23T15:44:55.350845Z","shell.execute_reply.started":"2024-10-23T15:44:55.170559Z","shell.execute_reply":"2024-10-23T15:44:55.349533Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Through this process, we ensure that the dataset is robust and capable of handling unseen categories, which ultimately enhances the reliability and accuracy of the machine learning models trained on this data. **Feel free to adjust any details to better suit your specific project or findings!**","metadata":{}},{"cell_type":"markdown","source":"### Analyzing Price Trends by Model Year and Vehicle Age\n\nIn this section, we explore the relationship between vehicle prices, model year, and vehicle age. We visualize the average price of vehicles by model year and analyze how age impacts the average price. Based on our findings, we will create a new column for vehicle age and standardize both age and mileage for further analysis.\n\n#### Step 1: Visualization of Average Price by Model Year\n\nTo begin, we calculate the average price of vehicles grouped by their model year. This visualization helps us understand how vehicle prices have evolved over the years.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 8))\navg_price_by_year = train.groupby(\"model_year\")[\"price\"].mean().reset_index()\nsns.barplot(x=\"model_year\", y=\"price\", data=avg_price_by_year)\nplt.xlabel(\"Model Year\")\nplt.ylabel(\"Average Price\")\nplt.title(\"Average Price by Model Year\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:55.352442Z","iopub.execute_input":"2024-10-23T15:44:55.352844Z","iopub.status.idle":"2024-10-23T15:44:55.893629Z","shell.execute_reply.started":"2024-10-23T15:44:55.352804Z","shell.execute_reply":"2024-10-23T15:44:55.89224Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Step 2: Visualization of Average Price by Vehicle Age\nNext, we analyze the average price of vehicles based on their age, calculated as the difference between the current year (2024) and the model year. This inverted trend helps us identify how vehicle depreciation impacts pricing.","metadata":{}},{"cell_type":"code","source":"df = train[[\"model_year\", \"price\"]]\ndf[\"age\"] = 2024 - df[\"model_year\"]\navg_price_by_age = df.groupby(\"age\")[\"price\"].mean().reset_index()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"age\", y=\"price\", data=avg_price_by_age)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Average Price\")\nplt.title(\"Average Price by Age\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:55.898103Z","iopub.execute_input":"2024-10-23T15:44:55.898549Z","iopub.status.idle":"2024-10-23T15:44:56.407174Z","shell.execute_reply.started":"2024-10-23T15:44:55.898506Z","shell.execute_reply":"2024-10-23T15:44:56.40579Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Step 3: Creating an Age Column and Transformations\nObserving the trends from the visualizations, we conclude that vehicle age has a significant impact on price. To facilitate further analysis, we will create a new column for vehicle age in both the training and test datasets. Additionally, we will drop the model_year column, as it is no longer needed.","metadata":{"execution":{"iopub.status.busy":"2024-06-14T08:39:07.758799Z","iopub.execute_input":"2024-06-14T08:39:07.759348Z","iopub.status.idle":"2024-06-14T08:39:07.765882Z","shell.execute_reply.started":"2024-06-14T08:39:07.75931Z","shell.execute_reply":"2024-06-14T08:39:07.764457Z"}}},{"cell_type":"code","source":"# Creating the 'age' column and dropping 'model_year'\ntrain[\"age\"] = 2024 - train[\"model_year\"]\ntrain.drop(\"model_year\", axis=1, inplace=True)\n\ntest[\"age\"] = 2024 - test[\"model_year\"]\ntest.drop(\"model_year\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:56.408563Z","iopub.execute_input":"2024-10-23T15:44:56.408954Z","iopub.status.idle":"2024-10-23T15:44:56.438438Z","shell.execute_reply.started":"2024-10-23T15:44:56.408901Z","shell.execute_reply":"2024-10-23T15:44:56.437237Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Through these analyses, we gain insights into how vehicle pricing is influenced by model year and age. The creation of the age column and subsequent standardization of relevant features prepare the dataset for effective modeling and predictions.","metadata":{}},{"cell_type":"markdown","source":"### Engine Data Extraction and Imputation\n\nIn this section, we address the issue of missing values in the `horsepower` and `capacity` features extracted from the `engine` column. Given the high variability in unique engine types, traditional imputation methods such as clustering or classification were deemed impractical. Instead, we opted for a methodical approach:\n\n1. **Extraction of Key Features**: We extract `horsepower` and `capacity` from the `engine` strings using regular expressions. This allows us to convert the textual engine descriptions into numerical features that can be more easily analyzed.\n\n2. **Imputation of Missing Values**: To fill in any missing values for `horsepower` and `capacity`, we calculate the median values based on groups defined by `brand`, `model`, and `fuel_type`. This ensures that the imputation is contextually relevant and reduces the risk of introducing bias.\n\n3. **Cleanup**: After extraction and imputation, we remove the original `engine` column, as it is no longer needed.\n\nThe following code implements this process:","metadata":{}},{"cell_type":"code","source":"# Count unique engines\nprint(f\"The unique engines in train: {train['engine'].nunique()}\")\nprint(f\"The unique engines in test: {test['engine'].nunique()}\")\n\n# Functions to extract horsepower and displacement\ndef extract_horsepower(engine_string):\n    match = re.search(r'(\\d+\\.?\\d*)\\s*HP', engine_string, re.IGNORECASE)\n    return float(match.group(1)) if match else np.nan\n\ndef extract_capacity(engine_string):\n    match = re.search(r'(\\d+\\.?\\d*)\\s*L', engine_string, re.IGNORECASE)\n    return float(match.group(1)) if match else np.nan\n\n# Apply extraction functions\nfor df in [train, test]:\n    df['horsepower'] = df['engine'].apply(lambda x: extract_horsepower(str(x)))\n    df['capacity'] = df['engine'].apply(lambda x: extract_capacity(str(x)))\n\n# Helper function to fill missing values based on median of similar rows\ndef impute_missing_values(df, group_cols, target_col):\n    # Calculate the median values for each group\n    medians = df.groupby(group_cols)[target_col].median()\n    \n    # Function to apply the median value\n    def fill_value(row):\n        if pd.isnull(row[target_col]):\n            group_values = tuple(row[group_col] for group_col in group_cols)\n            return medians[group_values] if group_values in medians else np.nan\n        return row[target_col]\n    \n    # Apply the function to fill missing values\n    df[target_col] = df.apply(fill_value, axis=1)\n\n# Define columns to group by for median calculation\ngroup_columns = ['brand', 'model', 'fuel_type']\n\n# Impute missing values for 'horsepower' and 'capacity'\nfor column in ['horsepower', 'capacity']:\n    impute_missing_values(train, group_columns, column)\n    impute_missing_values(test, group_columns, column)\n\n# Remove the engine column and check for null values\ntrain.drop(columns=[\"engine\"], inplace=True)\ntest.drop(columns=[\"engine\"], inplace=True)\n\n# Number of missing values\nprint(\"Missing values in the train data:\")\nprint(train.isna().sum())\nprint()\nprint(\"---------------------------------------------------\")\nprint()\nprint(\"Missing values in the test data:\")\nprint(test.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:56.440104Z","iopub.execute_input":"2024-10-23T15:44:56.440537Z","iopub.status.idle":"2024-10-23T15:44:59.90834Z","shell.execute_reply.started":"2024-10-23T15:44:56.440493Z","shell.execute_reply":"2024-10-23T15:44:59.906766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### KNN-Based Imputation for Missing Values\n\nIn this section, we employ **K-Nearest Neighbors (KNN) regression** to impute missing values for the `horsepower` and `capacity` features in our dataset. This approach is advantageous because it leverages the similarity between instances, making it effective for predicting missing values when relationships between features are complex and nonlinear.\n\n#### Steps Implemented:\n\n1. **Feature Selection**:\n    - The relevant features used for imputation are: \n      - `brand`\n      - `model`\n      - `milage`\n      - `fuel_type`\n      - `transmission`\n      - `age`\n    - These features were selected because they have a significant influence on both `horsepower` and `capacity`.\n\n2. **Data Preprocessing**:\n    - **One-Hot Encoding**: We applied one-hot encoding to categorical features (`brand`, `model`, `fuel_type`, and `transmission`).\n    - **Scaling**: We scaled numerical features (`milage` and `age`) using `StandardScaler` to ensure that all features have a similar range, which is essential for KNN performance.\n\n3. **Model Training and Evaluation**:\n    - We split the dataset into **training** and **validation** sets to train our KNN model on the available data where the target variable is not missing.\n    - **Elbow Point Analysis**: We evaluated the model using different values for the number of neighbors (k). The elbow method helps in selecting the optimal number of neighbors that minimizes the Root Mean Squared Error (RMSE) for `horsepower` and `capacity`.\n    - We selected the optimal k based on this analysis for both target variables.\n\n4. **Imputation**:\n    - After identifying the best KNN model configurations for both `horsepower` and `capacity`, we used these models to predict and impute the missing values.\n    - This imputation is performed on both the training and test datasets, ensuring that the predicted values are contextually consistent, as they are based on the nearest neighbors in the feature space.\n\nThis approach helps maintain data consistency while leveraging feature similarities to predict missing values accurately.","metadata":{}},{"cell_type":"code","source":"# Define a function to train a KNN regressor model\ndef train_knn_regressor(X_train, y_train, n_neighbors):\n    knn = KNeighborsRegressor(n_neighbors=n_neighbors)\n    knn.fit(X_train, y_train)\n    return knn\n\n# Function to evaluate the model\ndef evaluate_model(model, X_val, y_val):\n    y_pred = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    return rmse\n\n# Specify feature columns\nfeature_cols = [\"brand\", \"model\", \"milage\", \"fuel_type\", \"transmission\", \"age\"]\n\n# Define the ColumnTransformer for preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), ['milage', 'age']),  # Scale numerical features\n        ('cat', OneHotEncoder(handle_unknown='ignore'), ['brand', 'model', 'fuel_type', 'transmission'])  # One-hot encode categorical features\n    ],\n    remainder='drop'  # Drop any columns not specified\n)\n\n# Imputation for 'horsepower'\ndf_train = train[train['horsepower'].notna()]\nX = df_train[feature_cols]\ny = df_train['horsepower']\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Create a pipeline for preprocessing and model training\ndef create_pipeline():\n    return Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('knn', KNeighborsRegressor())\n    ])\n\n# Perform elbow point analysis for horsepower\nneighbors = range(1, 10)\nerrors = []\nfor n in neighbors:\n    knn_horsepower_model = create_pipeline()\n    knn_horsepower_model.set_params(knn__n_neighbors=n)  # Set the number of neighbors\n    knn_horsepower_model.fit(X_train, y_train)  # Fit the model\n    rmse = evaluate_model(knn_horsepower_model, X_val, y_val)\n    errors.append(rmse)\n\n# Plotting the elbow curve for horsepower\nplt.figure(figsize=(10, 6))\nplt.plot(neighbors, errors, marker='o')\nplt.title('Elbow Point Analysis for KNN Regressor (Horsepower)')\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Root Mean Squared Error')\nplt.xticks(neighbors)\nplt.grid(True)\nplt.show()\n\n# Find the optimal number of neighbors (elbow point) for horsepower\noptimal_neighbors_horsepower = neighbors[errors.index(min(errors))]\nprint(f\"The optimal number of neighbors for horsepower is: {optimal_neighbors_horsepower}\")\n\n# Train the KNN model to predict horsepower using the optimal number of neighbors\nknn_horsepower_model = create_pipeline()\nknn_horsepower_model.set_params(knn__n_neighbors=optimal_neighbors_horsepower)\nknn_horsepower_model.fit(X_train, y_train)\n\n# Impute missing horsepower values\ndef predict_and_impute(df, model, target_col, feature_cols):\n    df_missing = df[df[target_col].isna()]\n    if not df_missing.empty:\n        X_missing = df_missing[feature_cols]\n        df.loc[df[target_col].isna(), target_col] = model.predict(X_missing)\n    return df\n\ntrain = predict_and_impute(train, knn_horsepower_model, 'horsepower', feature_cols)\ntest = predict_and_impute(test, knn_horsepower_model, 'horsepower', feature_cols)\n\n# Imputation for 'capacity'\ndf_train = train[train['capacity'].notna()]\nX = df_train[feature_cols]\ny = df_train['capacity']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Perform elbow point analysis for capacity\nerrors = []\nfor n in neighbors:\n    knn_capacity_model = create_pipeline()\n    knn_capacity_model.set_params(knn__n_neighbors=n)  # Set the number of neighbors\n    knn_capacity_model.fit(X_train, y_train)  # Fit the model\n    mse = evaluate_model(knn_capacity_model, X_val, y_val)\n    errors.append(mse)\n\n# Plotting the elbow curve for capacity\nplt.figure(figsize=(10, 6))\nplt.plot(neighbors, errors, marker='o')\nplt.title('Elbow Point Analysis for KNN Regressor (Capacity)')\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Mean Squared Error')\nplt.xticks(neighbors)\nplt.grid(True)\nplt.show()\n\n# Find the optimal number of neighbors (elbow point) for capacity\noptimal_neighbors_capacity = neighbors[errors.index(min(errors))]\nprint(f\"The optimal number of neighbors for capacity is: {optimal_neighbors_capacity}\")\n\n# Train the KNN model to predict capacity using the optimal number of neighbors\nknn_capacity_model = create_pipeline()\nknn_capacity_model.set_params(knn__n_neighbors=optimal_neighbors_capacity)\nknn_capacity_model.fit(X_train, y_train)\n\n# Impute missing capacity values\ntrain = predict_and_impute(train, knn_capacity_model, 'capacity', feature_cols)\ntest = predict_and_impute(test, knn_capacity_model, 'capacity', feature_cols)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:44:59.910302Z","iopub.execute_input":"2024-10-23T15:44:59.910835Z","iopub.status.idle":"2024-10-23T15:48:14.791872Z","shell.execute_reply.started":"2024-10-23T15:44:59.910793Z","shell.execute_reply":"2024-10-23T15:48:14.790853Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Price Transformation\n\nBefore proceeding with model training and evaluation, we must address the distribution of the target variable, **`price`**. It is common for target variables in datasets to exhibit skewness, which can adversely affect the performance of regression models. To mitigate this issue, we apply a log transformation followed by capping.\n\n### Log Transformation\n\nThe **log transformation** is effective in reducing skewness, helping to make the distribution of the target variable more Gaussian-like. By applying this transformation, we can quantify the improvement in distribution by calculating the skewness of the transformed prices. This allows us to assess whether the transformation has successfully normalized the data.\n\n### Capping\n\nAfter applying the log transformation, we cap the prices at a value of 13. This step helps to further reduce the influence of extreme outliers on our model, ensuring a more stable and reliable analysis. Capping prevents the skewed data points from disproportionately affecting the overall distribution and model performance.\n\nThrough these steps, we aim to prepare the `price` variable for more effective modeling, facilitating improved regression outcomes.","metadata":{}},{"cell_type":"code","source":"# Calculate the log transformation\nlog_price = np.log(train[\"price\"])\n\n# Cap the log-transformed price to 13\ncapped_log_price = np.clip(log_price, None, 14)\n\n# Calculate skewness\noriginal_log_skewness = skew(log_price)\ncapped_log_skewness = skew(capped_log_price)\n\n# Create a figure with two subplots (1 row, 2 columns)\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot the histogram for log-transformed price before capping\nsns.histplot(log_price, kde=True, ax=axes[0])\naxes[0].set_title('Log of Price (Before Capping)')\naxes[0].set_xlabel('Log(Price)')\naxes[0].set_ylabel('Frequency')\naxes[0].text(0.95, 0.95, f'Skewness: {original_log_skewness:.2f}', \n             transform=axes[0].transAxes, fontsize=12, \n             verticalalignment='top', horizontalalignment='right')\n\n# Plot the histogram for capped log-transformed price\nsns.histplot(capped_log_price, kde=True, ax=axes[1])\naxes[1].set_title('Capped Log of Price (After Capping)')\naxes[1].set_xlabel('Capped Log(Price)')\naxes[1].set_ylabel('Frequency')\naxes[1].text(0.95, 0.95, f'Skewness: {capped_log_skewness:.2f}', \n             transform=axes[1].transAxes, fontsize=12, \n             verticalalignment='top', horizontalalignment='right')\n\n# Adjust layout for better spacing\nplt.tight_layout()\nplt.show()\n\n# Update the 'price' column with the capped log transformation\ntrain[\"price\"] = capped_log_price","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:48:14.793606Z","iopub.execute_input":"2024-10-23T15:48:14.794255Z","iopub.status.idle":"2024-10-23T15:48:16.383026Z","shell.execute_reply.started":"2024-10-23T15:48:14.79421Z","shell.execute_reply":"2024-10-23T15:48:16.381733Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Preprocessing: Target Encoding, One-Hot Encoding, and Scaling\n\nIn this section, we perform several preprocessing steps on the training and test datasets to prepare the data for model building. These steps include **target encoding**, **one-hot encoding**, and **scaling**.\n\n#### 1. Target Encoding:\n- **Target encoding** is applied to categorical columns where the categories are encoded based on the target variable (`price`). This method is particularly useful when the categorical features have a high cardinality.\n- We apply target encoding on the following columns: `brand`, `model`, `transmission`, `int_col`, and `ext_col`.\n- The target encoding is fitted on the training data and then applied to both the training and test datasets to ensure consistency.\n\n#### 2. One-Hot Encoding:\n- We apply **one-hot encoding** to categorical features with fewer categories. Specifically, we encode `fuel_type` and `accident` using one-hot encoding, which converts these features into binary columns.\n- The `drop_first=True` argument is used to avoid multicollinearity by dropping the first category of each feature.\n\n#### 3. Min-Max Scaling:\n- We perform **Min-Max scaling** on the numerical features (`milage`, `age`, `horsepower`, and `capacity`) to scale the values between 0 and 1. This ensures that the features are on a comparable scale, which helps improve the performance of machine learning models, particularly those sensitive to feature scaling like KNN.\n- The scaler is fitted on the training data and then applied to both the training and test datasets.\n\nThis preprocessing pipeline prepares the data for robust model building by transforming the categorical and numerical features appropriately.","metadata":{}},{"cell_type":"code","source":"from category_encoders import TargetEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize TargetEncoder for specified columns\ntarget_columns = ['brand', 'model', 'transmission', 'int_col', 'ext_col']\ntarget_encoder = TargetEncoder(cols=target_columns)\n\n# Perform target encoding on training data\ntrain_encoded = target_encoder.fit_transform(train[target_columns], train['price'])\n\n# Replace original categorical columns with their target-encoded values in training data\ntrain.update(train_encoded)\n\n# One-hot encoding for 'fuel_type' and 'accident' in training data\none_hot_columns = ['fuel_type', 'accident']\ntrain = pd.get_dummies(train, columns=one_hot_columns, drop_first=True)\n\n# Min-Max Scaling for numerical columns in training data\nmin_max_columns = ['milage', 'age', 'horsepower', 'capacity']\nscaler = MinMaxScaler()\n\n# Fit and transform the scaler on the training data\ntrain[min_max_columns] = scaler.fit_transform(train[min_max_columns])\n\n# Perform target encoding on test data using the same encoder\ntest_encoded = target_encoder.transform(test[target_columns])\n\n# Replace original categorical columns with their target-encoded values in test data\ntest.update(test_encoded)\n\n# One-hot encoding for 'fuel_type' and 'accident' in test data\ntest = pd.get_dummies(test, columns=one_hot_columns, drop_first=True)\n\n# Min-Max Scaling for numerical columns in test data using the same scaler\ntest[min_max_columns] = scaler.transform(test[min_max_columns])\n\ntrain = train.astype('float')\ntest = test.astype('float')\n\n# Columns to be standardized\nstandardize_columns = ['brand', 'model', 'transmission', 'ext_col', 'int_col']\n\n# Initialize StandardScaler\nscaler = StandardScaler()\n\n# Fit and transform the scaler on the specified columns in the training data\ntrain[standardize_columns] = scaler.fit_transform(train[standardize_columns])\n\n# Apply the same transformation to the test data\ntest[standardize_columns] = scaler.transform(test[standardize_columns])","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:48:16.384929Z","iopub.execute_input":"2024-10-23T15:48:16.385347Z","iopub.status.idle":"2024-10-23T15:48:16.920837Z","shell.execute_reply.started":"2024-10-23T15:48:16.385304Z","shell.execute_reply":"2024-10-23T15:48:16.919746Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modeling\n\nIn this section, we build and evaluate several machine learning models to predict car prices. The models we will explore include:\n\n- **Linear Regression**: A fundamental algorithm for regression tasks, which establishes a linear relationship between features and the target variable.\n- **Random Forest Regressor**: An ensemble method that utilizes multiple decision trees to improve prediction accuracy and control overfitting.\n- **Gradient Boosting Regressor**: Another ensemble technique that builds models sequentially to minimize errors, which often yields high predictive performance.\n- **Support Vector Regressor (SVR)**: A regression method that uses support vector machines to model complex relationships.\n- **XGBoost Regressor**: An efficient and scalable implementation of gradient boosting that is known for its performance in various machine learning competitions.\n- **AdaBoost Regressor**: An ensemble technique that combines multiple weak learners to create a strong learner, improving the overall accuracy.\n\nTo assess the performance of these models, we will use the Root Mean Squared Error (RMSE) as our evaluation metric. The following code demonstrates how to train these models, evaluate their performance, and visualize the results.\n","metadata":{}},{"cell_type":"code","source":"# Prepare the data\nX = train.drop('price',axis=1)\ny = train['price']\n\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Initialize models\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"Random Forest\": RandomForestRegressor(random_state=0),\n    \"Gradient Boosting\": GradientBoostingRegressor(random_state=0),\n    \"Support Vector Regression\": SVR(),\n    \"XGBoost\": XGBRegressor(eval_metric='rmse', random_state=0),\n    \"AdaBoost\": AdaBoostRegressor(random_state=0)\n}\n\n# Train and evaluate models\nrmse_results = {}\n\nfor model_name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    rmse_results[model_name] = rmse\n\n# Convert results to DataFrame for easier visualization\nrmse_df = pd.DataFrame(rmse_results.items(), columns=['Model', 'RMSE'])\n\n# Plotting the RMSE of different models\nplt.figure(figsize=(12, 6))\nsns.barplot(x='RMSE', y='Model', data=rmse_df, palette='viridis')\nplt.title('Model Comparison: RMSE of Different Regressors')\nplt.xlabel('Root Mean Squared Error')\nplt.ylabel('Model')\nplt.grid(axis='x')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:48:16.922301Z","iopub.execute_input":"2024-10-23T15:48:16.922678Z","iopub.status.idle":"2024-10-23T15:51:06.74196Z","shell.execute_reply.started":"2024-10-23T15:48:16.922641Z","shell.execute_reply":"2024-10-23T15:51:06.740629Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference from the Model Comparison (RMSE of Different Regressors)\n\nThe plot visualizes the Root Mean Squared Error (RMSE) for different regression models, used to predict the target variable (likely car prices, based on the feature set). From the bar chart, we can observe the following:\n\n### XGBoost\n- **XGBoost** shows the **lowest RMSE**, meaning it performs the best out of the models considered. \n- This indicates that XGBoost captures the patterns in the dataset more effectively compared to other models. \n- Thus, it is the best-performing model in this comparison.\n\n### Linear Regression and Random Forest\n- **Linear Regression** and **Random Forest** also perform well, with relatively low RMSE values. \n- These models may be simpler and easier to interpret but are still competitive in terms of performance.\n\n### Support Vector Regression (SVR)\n- **Support Vector Regression (SVR)** performs the worst, with a significantly higher RMSE.\n- This suggests that SVR is not as well-suited to the data or might require additional tuning (e.g., kernel selection or hyperparameter adjustment).\n\n### Gradient Boosting and AdaBoost\n- **Gradient Boosting** and **AdaBoost** fall in the middle of the spectrum.\n- They perform better than SVR but are not as effective as XGBoost, Random Forest, or Linear Regression.\n\n### Conclusion\n- The result implies that **XGBoost** is the most effective model for minimizing prediction error in this dataset.\n- XGBoost should be considered for further tuning and optimization, as it shows the best potential for improved performance.","metadata":{}},{"cell_type":"code","source":"# Define the parameter grid to search over\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0]\n}\n\n# Initialize the XGBoost regressor\nxgb = XGBRegressor(random_state=0, eval_metric='rmse')\n\n# Initialize the grid search with cross-validation\ngrid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, \n                           scoring='neg_mean_squared_error', \n                           cv=5, n_jobs=-1, verbose=1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\n\n# Retrieve and print the best parameters and score\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best RMSE Score:\", np.sqrt(-grid_search.best_score_))\n\n# Use the best estimator to make predictions\nbest_xgb = grid_search.best_estimator_\ny_pred_best = best_xgb.predict(X_val)\nrmse_best = np.sqrt(mean_squared_error(y_val, y_pred_best))\nprint(\"RMSE on Validation Set with Best Estimator:\", rmse_best)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:51:06.743765Z","iopub.execute_input":"2024-10-23T15:51:06.744286Z","iopub.status.idle":"2024-10-23T15:57:58.429902Z","shell.execute_reply.started":"2024-10-23T15:51:06.744233Z","shell.execute_reply":"2024-10-23T15:57:58.42829Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Importance Analysis with XGBoost\n\nAfter training the XGBoost model using the best parameters from the grid search, it's essential to analyze the importance of the features in contributing to the model's predictions. This can help with understanding the most impactful features for the model and guiding future feature selection or engineering.\n\nIn this section, we:\n\n1. **Retrieve Feature Importances**:\n   - Using the best-performing XGBoost model (`best_xgb`), we extract the feature importance scores, which rank features based on their contribution to reducing the loss function.\n\n2. **Create a DataFrame**:\n   - To make the importance scores more interpretable, we store the feature names and their importance values in a pandas DataFrame.\n\n3. **Sort by Importance**:\n   - We sort the features in descending order based on their importance scores to easily identify the most significant features.\n\n4. **Plot the Feature Importances**:\n   - A horizontal bar chart is used to visualize the feature importances, with the most important features displayed at the top. This helps in quickly understanding which features play a key role in model predictions.","metadata":{}},{"cell_type":"code","source":"# Get feature importance from the best estimator\nfeature_importances = best_xgb.feature_importances_\n\n# Create a DataFrame for better visualization\nimportance_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': feature_importances\n})\n\n# Sort the features by their importance\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# Display the top features\nprint(importance_df)\n\n# Plot the feature importances\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance from XGBoost')\nplt.gca().invert_yaxis()  # Invert y-axis to show the most important feature at the top\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:57:58.43168Z","iopub.execute_input":"2024-10-23T15:57:58.43208Z","iopub.status.idle":"2024-10-23T15:57:58.808007Z","shell.execute_reply.started":"2024-10-23T15:57:58.432037Z","shell.execute_reply":"2024-10-23T15:57:58.806752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Selection Based on Feature Importance\n\nIn this section, we refine our model by selecting only the most important features based on a predefined threshold for feature importance. This helps in simplifying the model and potentially improving performance by removing less relevant features.\n\n1. **Define a Threshold**:\n   - We define a threshold (in this case, `0.015`) to determine which features are considered important enough to be retained for further modeling.\n\n2. **Select Features**:\n   - Features with importance scores greater than the defined threshold are selected. These features are expected to have the most significant impact on the model's predictions.\n\n3. **Create a New Training Set**:\n   - A new training and validation set is created using only the selected features that passed the importance threshold.\n\n4. **Re-Train the Model**:\n   - The XGBoost model is re-trained using only the selected features.\n\n5. **Evaluate the Model**:\n   - The model is evaluated using the validation set, and the Root Mean Squared Error (RMSE) is computed to measure its performance with the reduced feature set.","metadata":{"execution":{"iopub.status.busy":"2024-10-23T11:53:06.465316Z","iopub.execute_input":"2024-10-23T11:53:06.465809Z","iopub.status.idle":"2024-10-23T11:53:06.478634Z","shell.execute_reply.started":"2024-10-23T11:53:06.465772Z","shell.execute_reply":"2024-10-23T11:53:06.476518Z"}}},{"cell_type":"code","source":"# Define a threshold for feature importance\nthreshold = 0.01\n\n# Select features that have importance above the threshold\nselected_features = importance_df[importance_df['Importance'] > threshold]['Feature'].tolist()\n\n# Create a new training set with only the selected features\nX_train_selected = X_train[selected_features]\nX_val_selected = X_val[selected_features]\n\n# Now you can re-train your model using only the selected features\nbest_xgb.fit(X_train_selected, y_train)\ny_pred_selected = best_xgb.predict(X_val_selected)\n\n# Evaluate the new model\nrmse_selected = np.sqrt(mean_squared_error(y_val, y_pred_selected))\nprint(\"RMSE with Selected Features:\", rmse_selected)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:57:58.809579Z","iopub.execute_input":"2024-10-23T15:57:58.809957Z","iopub.status.idle":"2024-10-23T15:57:59.323926Z","shell.execute_reply.started":"2024-10-23T15:57:58.809918Z","shell.execute_reply":"2024-10-23T15:57:59.32257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Learning Curves\nLearning curves are a valuable tool to visualize the performance of the model as the amount of training data increases. They can help identify whether the model is underfitting (high bias) or overfitting (high variance). In this section, we visualize the learning curves for the XGBoost model. The learning curves show the training and validation scores as the size of the training set increases. This helps us understand if the model is underfitting (training and validation scores are both low) or overfitting (training score is high while validation score is low).","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\n# Function to plot learning curves\ndef plot_learning_curves(model, X, y):\n    train_sizes, train_scores, test_scores = learning_curve(\n        model, X, y, cv=5, n_jobs=-1, \n        train_sizes=np.linspace(0.1, 1.0, 10)\n    )\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_sizes, train_scores_mean, label='Training Score', color='blue')\n    plt.plot(train_sizes, test_scores_mean, label='Cross-Validation Score', color='orange')\n    plt.title('Learning Curves')\n    plt.xlabel('Training Size')\n    plt.ylabel('Score')\n    plt.legend(loc='best')\n    plt.grid()\n    plt.show()\n\n# Plot learning curves for the best XGBoost model\nplot_learning_curves(best_xgb, X_train_selected, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:57:59.325567Z","iopub.execute_input":"2024-10-23T15:57:59.325946Z","iopub.status.idle":"2024-10-23T15:58:11.914647Z","shell.execute_reply.started":"2024-10-23T15:57:59.325907Z","shell.execute_reply":"2024-10-23T15:58:11.913415Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Error Analysis\nUnderstanding the errors made by the model can provide insights into areas for improvement. We can visualize the residuals to analyze where the model is making mistakes. In this section, we analyze the residuals of the predictions made by our model. By plotting the residuals against the predicted values, we can see if there are any patterns (which would indicate that the model is not capturing some relationships in the data). Additionally, summary statistics provide insights into the average error, variance, and range of errors.","metadata":{}},{"cell_type":"code","source":"# Calculate residuals\nresiduals = y_val - y_pred_best\n\n# Plotting residuals\nplt.figure(figsize=(10, 6))\nplt.scatter(y_val, residuals, alpha=0.5)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs. Predicted Values')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.grid()\nplt.show()\n\n# Summary statistics of residuals\nprint(\"Residuals Summary Statistics:\")\nprint(f\"Mean: {np.mean(residuals)}\")\nprint(f\"Standard Deviation: {np.std(residuals)}\")\nprint(f\"Max: {np.max(residuals)}\")\nprint(f\"Min: {np.min(residuals)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:58:11.916293Z","iopub.execute_input":"2024-10-23T15:58:11.916673Z","iopub.status.idle":"2024-10-23T15:58:12.287424Z","shell.execute_reply.started":"2024-10-23T15:58:11.916635Z","shell.execute_reply":"2024-10-23T15:58:12.286099Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assume y_val are the true values and y_pred are the predicted values\nresiduals = y_val - y_pred\n\n# Residuals vs Fitted plot\nplt.figure(figsize=(10, 6))\nplt.scatter(y_pred, residuals, alpha=0.5)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.title('Residuals vs Fitted')\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.show()\n\n# QQ plot for checking normality of residuals\nimport statsmodels.api as sm\nsm.qqplot(residuals, line='s')\nplt.title('QQ Plot of Residuals')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:58:12.289004Z","iopub.execute_input":"2024-10-23T15:58:12.289508Z","iopub.status.idle":"2024-10-23T15:58:14.106832Z","shell.execute_reply.started":"2024-10-23T15:58:12.289451Z","shell.execute_reply":"2024-10-23T15:58:14.105231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Quantile-Quantile (QQ) plot displayed above compares the sample quantiles of the residuals from a regression model with the theoretical quantiles of a standard normal distribution. The following points summarize the key insights from the plot:\n\n**`Linearity`**: The red line represents the ideal case where the residuals follow a perfect normal distribution. Deviations from this line indicate departures from normality.\n\n**`Deviations at the Tails`**: The plot shows significant deviations at both extremes (right and left tails), suggesting the presence of outliers or heavy-tailed residuals. These points deviate sharply from the theoretical line, indicating that the residuals are not normally distributed at the extremes.\n\n**`Central Fit`**: In the middle section of the plot, the residuals closely follow the red line, indicating that the residuals near the center of the distribution approximate normality reasonably well.\n\n**`Outliers`**: The points farthest from the red line, particularly at the upper and lower ends of the distribution, are likely outliers. These data points can have a substantial impact on model performance and may need further investigation.\n\n### Conclusion\nThis QQ plot suggests that the residuals deviate from normality, particularly in the tails, where the presence of outliers or non-normal distribution characteristics is evident. Addressing these outliers may improve model accuracy and adherence to the assumptions of normality required by certain regression models.","metadata":{}},{"cell_type":"markdown","source":"## Model Deployment\nTo deploy our model, we would typically save the model using a library like joblib or pickle, which allows us to serialize the model for future use.","metadata":{}},{"cell_type":"code","source":"!pip install joblib","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:58:14.109941Z","iopub.execute_input":"2024-10-23T15:58:14.111558Z","iopub.status.idle":"2024-10-23T15:58:27.230075Z","shell.execute_reply.started":"2024-10-23T15:58:14.111497Z","shell.execute_reply":"2024-10-23T15:58:27.228609Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Save the trained model\njoblib.dump(best_xgb, 'xgb_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:58:27.232272Z","iopub.execute_input":"2024-10-23T15:58:27.232823Z","iopub.status.idle":"2024-10-23T15:58:27.251329Z","shell.execute_reply.started":"2024-10-23T15:58:27.232764Z","shell.execute_reply":"2024-10-23T15:58:27.250186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Future Scope of Car Price Prediction Project\n\n1. **Outlier Detection and Removal**:  \n   Outliers can negatively impact the performance of machine learning models by distorting key statistical metrics like mean and standard deviation. Moving forward, systematic outlier detection techniques such as **Z-score analysis** or **IQR (Interquartile Range) method** can be used to identify extreme values. These values can either be removed or capped to reduce their influence on the model. This step will help in creating a more robust and generalizable predictive model.\n\n2. **Advanced Feature Selection**:  \n   The project can benefit from more sophisticated feature selection techniques to ensure that only the most informative features are used in model training. Methods such as **Recursive Feature Elimination (RFE)** and **LASSO (Least Absolute Shrinkage and Selection Operator)** can be employed to eliminate less significant features and reduce multicollinearity. Proper feature selection can improve model accuracy, reduce overfitting, and enhance computational efficiency.\n\n3. **Implementing Advanced Algorithms (Neural Networks)**:  \n   While traditional models like **KNN** and **Linear Regression** have been applied, future work could explore more advanced algorithms such as **Artificial Neural Networks (ANN)**. Neural networks, especially deep learning models, are capable of learning complex non-linear relationships in the data and could potentially offer better performance in predicting car prices. Leveraging neural networks could be particularly useful in handling large datasets with complex feature interactions.\n\n4. **Identifying Non-Linearity**:  \n   Non-linear relationships between predictors and the target variable may exist and go unnoticed if only linear models are used. To address this, **polynomial transformations** and non-linear models like **Support Vector Regression (SVR)** with a non-linear kernel could be explored. By identifying and modeling non-linear patterns, the model’s predictive capabilities could be significantly improved.\n\n5. **Pipeline Automation**:  \n   Automating the machine learning workflow through pipelines helps in making the process scalable and reproducible. The current implementation could be further enhanced by building custom preprocessing pipelines, including steps like **missing data imputation**, **scaling**, **feature engineering**, and **model selection**. This would allow for easy experimentation with different models and preprocessing techniques without having to manually tweak the code at each step.\n\n6. **Data Augmentation**:  \n   To address imbalances in certain categorical variables like 'brand' or 'fuel type', synthetic data augmentation techniques such as **SMOTE (Synthetic Minority Over-sampling Technique)** or **ADASYN** can be explored. These methods generate synthetic data points for underrepresented classes, which can help improve the model's performance on minority classes and provide a more balanced view of the data during training.\n\n7. **Exploring Feature Engineering**:  \n    As the project progresses, more advanced feature engineering techniques could be explored. This involves creating new features that provide additional insights into the data. For instance, features like \"mileage per year\", \"engine capacity to horsepower ratio\", or \"price-to-weight ratio\" could be derived based on domain knowledge and intuition. Effective feature engineering can help uncover hidden patterns in the data and significantly boost the model’s performance.","metadata":{}}]}